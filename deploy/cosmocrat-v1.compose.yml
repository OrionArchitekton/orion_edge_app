x-common-env: &common-env
  TZ: America/Los_Angeles

services:
  traefik:
    image: traefik:v3.1
    container_name: traefik
    ports:
      - "80:80"
      - "443:443"
    command:
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      # optional: redirect HTTPâ†’HTTPS
      - --entrypoints.web.http.redirections.entrypoint.to=websecure
      - --entrypoints.web.http.redirections.entrypoint.scheme=https
      # ACME (HTTP-01)
      - --certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL:-admin@orionbot.online}
      - --certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json
      - --certificatesresolvers.letsencrypt.acme.httpchallenge=true
      - --certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web
      # Docker provider
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      # (optional) logs
      - --log.level=INFO
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./letsencrypt:/letsencrypt
    restart: unless-stopped

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: cosmocrat
      POSTGRES_USER: cosmocrat
      POSTGRES_PASSWORD: change-me
    volumes:
      - pg_data:/var/lib/postgresql/data
    labels:
      - traefik.enable=false
    restart: unless-stopped

  redis:
    image: redis:7
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    labels:
      - traefik.enable=false
    restart: unless-stopped

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    environment:
      <<: *common-env
      CLICKHOUSE_DB: langfuse
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: change-me
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --user $$CLICKHOUSE_USER --password $$CLICKHOUSE_PASSWORD -q 'select 1'"]
      interval: 10s
      timeout: 3s
      retries: 10
    labels:
      - traefik.enable=false
    restart: unless-stopped

  langfuse:
    image: ghcr.io/langfuse/langfuse:2
    environment:
      <<: *common-env
      NEXT_PUBLIC_SIGNUP_DISABLED: "true"
      # v2: Only needs Postgres, no ClickHouse required
      DATABASE_URL: postgresql://cosmocrat:change-me@postgres:5432/cosmocrat
      NEXTAUTH_URL: http://ops.localhost/langfuse
      NEXTAUTH_SECRET: replace-me
      SALT: replace-me
      NODE_ENV: production
      PORT: "3000"
    depends_on:
      postgres:
        condition: service_started
    labels:
      - traefik.enable=true
      - traefik.http.routers.langfuse.rule=Host(`ops.localhost`) && PathPrefix(`/langfuse`)
      - traefik.http.middlewares.langfuse-strip.stripprefix.prefixes=/langfuse
      - traefik.http.routers.langfuse.middlewares=langfuse-strip
      - traefik.http.services.langfuse.loadbalancer.server.port=3000
    restart: unless-stopped

  n8n:
    image: n8nio/n8n:latest
    environment:
      <<: *common-env
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: admin
      N8N_BASIC_AUTH_PASSWORD: change-me
      N8N_HOST: ops.localhost
      N8N_PORT: 5678
    labels:
      - traefik.enable=true
      - traefik.http.routers.n8n.rule=Host(`ops.localhost`) && PathPrefix(`/n8n`)
      - traefik.http.middlewares.n8n-strip.stripprefix.prefixes=/n8n
      - traefik.http.routers.n8n.middlewares=n8n-strip
      - traefik.http.services.n8n.loadbalancer.server.port=5678
    volumes:
      - n8n_data:/home/node/.n8n
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - /opt/orion/models:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    labels:
      - traefik.enable=true
      - traefik.http.routers.ollama.rule=Host(`ops.localhost`) && PathPrefix(`/llm`)
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - traefik.http.middlewares.ollama-strip.stripprefix.prefixes=/llm
      - traefik.http.routers.ollama.middlewares=ollama-strip

  # vLLM disabled - using Ollama for CPU inference instead
  # vllm:
  #   image: vllm/vllm-openai:0.6.2.post1-cpu
  #   command:
  #     - "--model"
  #     - "Qwen/Qwen2.5-0.5B-Instruct"
  #     - "--host"
  #     - "0.0.0.0"
  #     - "--port"
  #     - "8000"
  #     - "--max-model-len"
  #     - "4096"
  #     - "--enforce-eager"
  #   environment:
  #     - VLLM_LOGGING_LEVEL=INFO
  #     - VLLM_TARGET_DEVICE=cpu
  #     - VLLM_USE_RAY=0
  #     - VLLM_DISABLE_CUSTOM_OPS=1
  #     - CUDA_VISIBLE_DEVICES=""
  #     - HF_HOME=/models
  #     - HF_HUB_ENABLE_HF_TRANSFER=1
  #   volumes:
  #     - /opt/orion/models:/models
  #   ports:
  #     - "8000:8000"
  #   labels:
  #     - traefik.enable=true
  #     - traefik.http.routers.vllm.rule=Host(`ops.localhost`) && PathPrefix(`/vllm`)
  #     - traefik.http.middlewares.vllm-strip.stripprefix.prefixes=/vllm
  #     - traefik.http.routers.vllm.middlewares=vllm-strip
  #     - traefik.http.services.vllm.loadbalancer.server.port=8000
  #   restart: unless-stopped

  mcp:
    build:
      context: ./app/mcp
      dockerfile: Dockerfile
    environment:
      <<: *common-env
      POSTGRES_URL: postgresql://cosmocrat:change-me@postgres:5432/cosmocrat
      REDIS_URL: redis://redis:6379/0
      LANGFUSE_BASE_URL: http://langfuse:3000
      LANGFUSE_API_KEY: replace-me
      OPENAI_BASE_URL: http://ollama:11434/v1
      OPENAI_API_KEY: ollama
      OPENAI_MODEL: qwen2.5:0.5b-instruct
    depends_on: [postgres, redis, langfuse, ollama]
    labels:
      - traefik.enable=true
      - traefik.http.routers.mcp.rule=Host(`mcp.localhost`)
      - traefik.http.services.mcp.loadbalancer.server.port=8080
    restart: unless-stopped

  memory-consolidator:
    image: python:3.11-slim
    volumes:
      - ./jobs/memory:/app:ro
    working_dir: /app
    command: ["sh", "-c", "pip install --no-cache-dir --quiet requests 'psycopg[binary]' >/dev/null 2>&1 && while true; do python /app/consolidate.py; echo '[INFO] Sleeping for 24 hours until next run...'; sleep 86400; done"]
    environment:
      <<: *common-env
      REDIS_HOST: redis
      REDIS_PORT: 6379
      PG_CONN_STRING: postgres://cosmocrat:change-me@postgres:5432/cosmocrat
      POSTGRES_URL: postgresql://cosmocrat:change-me@postgres:5432/cosmocrat
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-}
      LANGFUSE_BASE_URL: http://langfuse:3000
      LANGFUSE_API_KEY: ${LANGFUSE_API_KEY:-replace-me}
    depends_on: [postgres, redis, langfuse]
    restart: unless-stopped

  runner-daily-report:
    image: python:3.11-slim
    volumes:
      - ./jobs/runner:/app:ro
      - ./data/reports:/data/reports
      - ./data/conversations:/data/conversations
    working_dir: /app
    command: ["sh", "-c", "pip install --no-cache-dir requests && python3 /app/runner.py --daily"]
    environment:
      <<: *common-env
      EXEC_ANALYST_URL: https://mcp.orionbot.online/api/analyst
      # MCP_BASE_URL protocol is configurable via MCP_PROTOCOL (default: http).
      # If MCP is served over HTTPS or via a TLS proxy, set MCP_PROTOCOL=https.
      MCP_PROTOCOL: ${MCP_PROTOCOL:-http}
      MCP_BASE_URL: ${MCP_PROTOCOL}://mcp:8080
      STORAGE_PATH: /data/reports
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-}
      JOB_NAME: exec-daily-v3
      OPENAI_BASE_URL: http://ollama:11434/v1
      OPENAI_API_KEY: ollama
      OPENAI_MODEL: qwen2.5:0.5b-instruct
    depends_on: [mcp]
    restart: unless-stopped

  slack-hooks:
    image: node:20-alpine
    working_dir: /app
    volumes:
      - ../scripts/slack_csv_hooks:/app
      - ../templates:/templates:ro
    command: ["node", "server.js"]
    environment:
      - PORT=3000
      - SLACK_SIGNING_SECRET=${SLACK_SIGNING_SECRET:-}
      - DEFAULT_SLACK_WEBHOOK=${DEFAULT_SLACK_WEBHOOK:-}
    labels:
      - traefik.enable=true
      - traefik.http.routers.slack-hooks.rule=Host(`slack.orionbot.online`) && PathPrefix(`/slack`)
      - traefik.http.routers.slack-hooks.entrypoints=websecure
      - traefik.http.routers.slack-hooks.tls.certresolver=letsencrypt
      - traefik.http.services.slack-hooks.loadbalancer.server.port=3000
      - traefik.http.middlewares.slack-strip.stripprefix.prefixes=/slack
      - traefik.http.routers.slack-hooks.middlewares=slack-strip
    restart: unless-stopped

  slack-agent:
    image: node:20-alpine
    working_dir: /app
    volumes:
      - ../scripts/slack_agent:/app
      - ../scripts/slack_oauth:/slack_oauth:ro
      - slack_agent_node_modules:/app/node_modules
    command: ["sh", "-c", "npm install && node -r dotenv/config app.js"]
    environment:
      <<: *common-env
      SLACK_APP_TOKEN: ${SLACK_APP_TOKEN:-}
      SLACK_BOT_TOKEN: ${SLACK_BOT_TOKEN:-}
      MCP_BASE_URL: http://mcp:8080
      OPENAI_BASE_URL: http://ollama:11434/v1
      OPENAI_API_KEY: ollama
      OPENAI_MODEL: qwen2.5:0.5b-instruct
      WEBHOOK_SIGNING_SECRET: ${WEBHOOK_SIGNING_SECRET:-}
      DEFAULT_PLAN_CHANNEL: ${DEFAULT_PLAN_CHANNEL:-#proj-chatbot}
    depends_on: [mcp, ollama]
    restart: unless-stopped

  slack-resolve-channels:
    image: node:20-alpine
    working_dir: /app
    volumes:
      - ../scripts/slack_agent:/app
      - ../scripts/slack_oauth:/slack_oauth:ro
      - slack_agent_node_modules:/app/node_modules
    command: ["sh", "-c", "npm install && node -r dotenv/config resolve-channels.js"]
    environment:
      <<: *common-env
      SLACK_BOT_TOKEN: ${SLACK_BOT_TOKEN:-}
    profiles:
      - tools  # Only runs when explicitly requested: docker compose --profile tools run slack-resolve-channels
    restart: "no"

volumes:
  pg_data:
  redis_data:
  n8n_data:
  clickhouse_data:
  slack_hooks_node_modules:
  slack_agent_node_modules:
